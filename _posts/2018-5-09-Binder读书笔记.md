## 源码环境：frameworks_4.0.4 ##
## 参考资料：«深入理解Android：卷I»，«深入理解Android：卷II»,«Android的设计与实现：卷I»##

## Binder ##
**读书笔记，仅仅是为了记录个人了解，并不作为参考文章。**
 
Android进程间通信使用的是Binder机制,framework中Binder的应用很广，熟悉Binder对学习framework有很大帮助。

Android启动之后，会启动一个SystemServer进程，很多系统服务都运行在这个进程中，例如**ActivityManagerService**、**WindowManagerService**，这里由ActivityManagerService为入口了解一下Binder。  

ServiceManager
==

**ServiceManager是系统中service的总管，负责其他service的注册，管理。**    
serviceManager在init.rc中启动，比Zygote更早启动。首先了解一下ServiceManager。    
**frameworks\base\cmds\servicemanager\service_manager.c**
{% highlight java %}

//binder_state结构体
struct binder_state{
    int fd;//保存open系统调用返回的文件描述符
    void *mapped;//mmap系统调用返回的映射区的起始地址
    unsigned mapsize;//映射区的大小
};
{% endhighlight %}

**ServiceManager的入口函数。**
{% highlight c++ %}
//入口main函数
int main(int argc, char **argv){
    struct binder_state *bs;
    void *svcmgr = BINDER_SERVICE_MANAGER;
    //open Binder驱动
    bs = binder_open(128*1024);
    //将servicemanager注册为context manager,调用到Binder_Driver binder_ioctl
    if (binder_become_context_manager(bs)) {
        return -1;
    }
    svcmgr_handle = svcmgr;
	//无限循环 等待client通信
	//svcmgr_handler是一个方法下面会说
    binder_loop(bs, svcmgr_handler);
    return 0;
}
{% endhighlight %}
**binder_open函数**
{% highlight c++ %}
struct binder_state *binder_open(unsigned mapsize){
    struct binder_state *bs;
    bs = malloc(sizeof(*bs));//分配一块内存地址
    //...
    
	//通过open系统调用以读写方式打开设备文件
    bs->fd = open("/dev/binder", O_RDWR);//调用到Binder_Driver的binder_open函数
    -------------------------------------
    //...
    bs->mapsize = mapsize;
	//mmap系统调用的结果是进程空间的某个内存区域和内核空间的某个内存区域建立了映射关系
	//通过mmap系统调用将设备文件映射到当前进程的虚拟地址空间
    bs->mapped = mmap(NULL, mapsize, PROT_READ, MAP_PRIVATE, bs->fd, 0);
    return bs;

    //...
    return 0;
}
{% endhighlight %}

{% highlight c++ %}

//binder_proc结构体保存了service所在进程的一些数据对象
//Binder_Driver binder_open方法,初始化
static int binder_open(struct inode *nodp, struct file *filp){   
	struct binder_proc *proc;
    //...
	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
	//...
	//保存打开Binder设备的进程信息，即servicemanager
	get_task_struct(current->group_leader);
	proc->tsk = current->group_leader;
	INIT_LIST_HEAD(&proc->todo);//初始化可执行任务链表
	init_waitqueue_head(&proc->wait);//初始化等待队列列表，用于切换current进程到wait状态
	proc->default_priority = task_nice(current);//记录进程默认优先级
	binder_lock(__func__);
	binder_stats_created(BINDER_STAT_PROC);
	hlist_add_head(&proc->proc_node, &binder_procs);//将proc的节点加入全局列表binder_procs中
	proc->pid = current->group_leader->pid;
	INIT_LIST_HEAD(&proc->delivered_death);
	filp->private_data = proc;//将proc存入filp结构体的private_date成员

	binder_unlock(__func__);
    //在/proc/binder/proc目录下创建Binder通信信息文件，文件以PID命名
	if (binder_debugfs_dir_entry_proc) {
		char strbuf[11];
		//...
	}

	return 0;
}
{% endhighlight %}

{% highlight c++ %}

int binder_become_context_manager(struct binder_state *bs){           
    //调用Linux系统函数ioctl，向Binder设备发送BINDER_SET_CONTEXT_MGR指令
    return ioctl(bs->fd, BINDER_SET_CONTEXT_MGR, 0);
}
{% endhighlight %}

{% highlight c++ %}

//Binder_Driver的binder_ioctl方法 上面调用ioctl就会调用到这里
static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
	int ret;
	struct binder_proc *proc = filp->private_data;//从filp中取出在binder_open阶段赋值的proc对象
    //...
	switch (cmd) {
	 //binder_ioctl_write_read 数据的写跟读2个操作
	case BINDER_WRITE_READ://BINDER_WRITE_READ命令
		ret = binder_ioctl_write_read(filp, cmd, arg, thread);
	    //...
		break;
	case BINDER_SET_MAX_THREADS://设置最大的线程数量
		//...
		break;
	case BINDER_SET_CONTEXT_MGR://设置serviceManager
		ret = binder_ioctl_set_ctx_mgr(filp);
	    //...
		break;
	case BINDER_THREAD_EXIT:
		//...
		break;
	case BINDER_VERSION: {
	    //...
		break;
	}
	default:
		ret = -EINVAL;
		goto err;
	}
	ret = 0;
    //...
	return ret;
}
{% endhighlight %}

{% highlight c++ %}

//全局的一个binder_node对象
static struct binder_node *binder_context_mgr_node;
//binder_node  就是用来管理每个Service 对象的结构体

//Binder_Driver 设置serviceManager
static int binder_ioctl_set_ctx_mgr(struct file *filp){
	int ret = 0;
	struct binder_proc *proc = filp->private_data;
	kuid_t curr_euid = current_euid();
	//...
	ret = security_binder_set_context_mgr(proc->tsk);
    //...
	//生成一个新的binder_node,保存在全局变量binder_context_mgr_node中。
	binder_context_mgr_node = binder_new_node(proc, 0, 0);
	if (binder_context_mgr_node == NULL) {
		ret = -ENOMEM;
		goto out;
	}
	//binder_node各种引计数
	binder_context_mgr_node->local_weak_refs++;
	binder_context_mgr_node->local_strong_refs++;
	binder_context_mgr_node->has_strong_ref = 1;
	binder_context_mgr_node->has_weak_ref = 1;
out:
	return ret;
}
{% endhighlight %}

{% highlight c++ %}
//设置成为context manager之后就开始工作了
void binder_loop(struct binder_state *bs, binder_handler func){
    int res;
    //BINDER_WRITE_READ指令需要接收一个binder_write_read类型的参数
    struct binder_write_read bwr;
    unsigned readbuf[32];
    bwr.write_size = 0;//write_size大小用来判断是否有数据要写,为0代表不需要写
    bwr.write_consumed = 0;
    bwr.write_buffer = 0;
    //BC_ENTER_LOOPER是Binder协议中的Binder Command指令，以BC_为前缀
    readbuf[0] = BC_ENTER_LOOPER;
    //第一次调用binder_write,注册Binder线程
    binder_write(bs, readbuf, sizeof(unsigned));
    //readSize为0 所以没有read write完毕继续走下面
    //无限循环
    for (;;) {
        bwr.read_size = sizeof(readbuf);//这里read_size不为0了
        bwr.read_consumed = 0;
        bwr.read_buffer = (unsigned) readbuf;
        //调用ioctl，将进入Binder Driver的binder_ioctl函数
        //该函数用于从Binder驱动中读取IPC请求数据。
        //循环调用ioctl
        res = ioctl(bs->fd, BINDER_WRITE_READ, &bwr);
        //...
        //调用binder_parse处理Binder请求
        res = binder_parse(bs, 0, readbuf, bwr.read_consumed, func);
        //...
    }

{% endhighlight %}

{% highlight c++ %}

//binder_parse方法
int binder_parse(struct binder_state *bs, struct binder_io *bio,
                 uint32_t *ptr, uint32_t size, binder_handler func){
    int r = 1;
    uint32_t *end = ptr + (size / 4);

    while (ptr < end) {
        uint32_t cmd = *ptr++;//读取BR指令
        //...
        switch(cmd) {
        case BR_NOOP:
            break;
        case BR_TRANSACTION_COMPLETE:
            break;
        case BR_INCREFS:
        case BR_ACQUIRE:
        case BR_RELEASE:
        case BR_DECREFS:
           //...
            break;
        //走这里
        case BR_TRANSACTION: {
            struct binder_txn *txn = (void *) ptr;
            //...
            if (func) {
                unsigned rdata[256/4];
                struct binder_io msg;//Binder驱动发送给当前进程的IPC数据
                struct binder_io reply;//要写入Binder驱动的IPC数据
                int res;
                bio_init(&reply, rdata, sizeof(rdata), 4);
                bio_init_from_txn(&msg, txn);
				//调用func处理BC_TRANSACTION指令，处理结果保存在reply中
                res = func(bs, txn, &msg, &reply);
				//将处理结果返回给Binder驱动程序,调用binderWrite方法,同样是调用binder_transacetion
                binder_send_reply(bs, &reply, txn->data, res);
            }
            ptr += sizeof(*txn) / sizeof(uint32_t);
            break;
        }
        case BR_REPLY: {
            struct binder_txn *txn = (void*) ptr;
             //...
            if (bio) {
                bio_init_from_txn(bio, txn);
                bio = 0;
            } else {
               //...
            }
            ptr += (sizeof(*txn) / sizeof(uint32_t));
            r = 0;
            break;
        }
    }

    return r;
}
{% endhighlight %}

{% highlight c++ %}




//处理业务的方法
int svcmgr_handler(struct binder_state *bs,
                   struct binder_txn *txn,
                   struct binder_io *msg,
                   struct binder_io *reply){
    struct svcinfo *si;
    uint16_t *s;
    unsigned len;
    void *ptr;
    uint32_t strict_policy;
   //...
   //检查Binder驱动传递的txn-target是否为本函数
    if (txn->target != svcmgr_handle)
        return -1;

    // Equivalent to Parcel::enforceInterface(), reading the RPC
    // header with the strict mode policy mask and the interface name.
    // Note that we ignore the strict_policy and don't propagate it
    // further (since we do no outbound RPCs anyway).
    //读取并校验传递过来的IPC数据
    strict_policy = bio_get_uint32(msg);
    s = bio_get_string16(msg, &len);
    if ((len != (sizeof(svcmgr_id) / 2)) ||
        memcmp(svcmgr_id, s, sizeof(svcmgr_id))) {
        fprintf(stderr,"invalid id %s\n", str8(s));
        return -1;
    }
   //Binder驱动程序在接收到添加或者检索Service的请求后，会在txn的code中写入相应的请求指令
    switch(txn->code) {
    //查找服务
    case SVC_MGR_GET_SERVICE:
    case SVC_MGR_CHECK_SERVICE:
        s = bio_get_string16(msg, &len);
        ptr = do_find_service(bs, s, len);
        if (!ptr)
            break;
        bio_put_ref(reply, ptr);
        return 0;
    //添加服务
    case SVC_MGR_ADD_SERVICE:
        s = bio_get_string16(msg, &len);
        ptr = bio_get_ref(msg);
        if (do_add_service(bs, s, len, ptr, txn->sender_euid))
            return -1;
        break;
    //遍历服务
    case SVC_MGR_LIST_SERVICES: {
        unsigned n = bio_get_uint32(msg);

        si = svclist;
        while ((n-- > 0) && si)
            si = si->next;
        if (si) {
            bio_put_string16(reply, si->name);
            return 0;
        }
        return -1;
    }
    default:
        return -1;
    }

    bio_put_uint32(reply, 0);
    return 0;
}

{% endhighlight %}

{% highlight c++ %}


//binder_write函数
int binder_write(struct binder_state *bs, void *data, unsigned len){
    struct binder_write_read bwr;//这里是新的一个binder_write_read
    int res;
    bwr.write_size = len;
    bwr.write_consumed = 0;
    bwr.write_buffer = (unsigned) data;
    bwr.read_size = 0;//read_size为0，Binder驱动中会判断该值,代表不需要读
    bwr.read_consumed = 0;
    bwr.read_buffer = 0;
	//用到了binder_open返回的bs结构体
    //调用ioctl，将进入Binder Driver的binder_ioctl函数,指令为BINDER_WRITE_READ
    res = ioctl(bs->fd, BINDER_WRITE_READ, &bwr);
    //...
    return res;
}

{% endhighlight %}

{% highlight c++ %}

//Binder Driver的binder_ioctl
static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg){
     //...
     switch(cmd){
     	case BINDER_WRITE_READ://BINDER_WRITE_READ命令
     	//调用binder_ioctl_write_read方法
		ret = binder_ioctl_write_read(filp, cmd, arg, thread);
	    //...
		break;
        //...
     }
}
{% endhighlight %}

{% highlight c++ %}

//binder_ioctl_write_read binder数据的写跟读
static int binder_ioctl_write_read(struct file *filp,
				unsigned int cmd, unsigned long arg,
				struct binder_thread *thread){
	int ret = 0;
	struct binder_proc *proc = filp->private_data;
	unsigned int size = _IOC_SIZE(cmd);
	//arg就是binder_write_read的指针
	void __user *ubuf = (void __user *)arg;
	struct binder_write_read bwr;

	//...
    //long copy_from_user(void *to, const void __user *from, unsigned long n)
	//参数1( void __user *to): 拷贝内核空间的地址指针
    //参数2(const void *from): 用户空间的地址指针
    //参数3(unsigned long n):  用户空间到内核空间的字节数
    //返回值: 成功返回 0,失败是返回还没有拷贝的字节数
	//
	//copy_from_user把数据从用户进程拷贝到内核
	if (copy_from_user(&bwr, ubuf, sizeof(bwr))) {
		ret = -EFAULT;
		goto out;
	}
   //...
   
   //write操作
   //只有write_size大于，才调用 binder_thread_write方法
	if (bwr.write_size > 0) {
		ret = binder_thread_write(proc, thread, bwr.write_buffer,bwr.write_size,
					  &bwr.write_consumed);
		trace_binder_write_done(ret);
		if (ret < 0) {
			bwr.read_consumed = 0;
			if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
				ret = -EFAULT;
			goto out;   
		}
	}
	//read操作
	//readSize>0才需要binder_thread_read
	if (bwr.read_size > 0) {
		//binder_thread_read方法，数据的读取核心
		//f_flags & O_NONBLOCK 是否是非阻塞，这是false 表示是阻塞的
		ret = binder_thread_read(proc, thread, bwr.read_buffer,bwr.read_size,
					 &bwr.read_consumed,filp->f_flags & O_NONBLOCK);
		trace_binder_read_done(ret);
		if (!list_empty(&proc->todo))
			wake_up_interruptible(&proc->wait);
		if (ret < 0) {
			if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
				ret = -EFAULT;
			goto out;
		}
	}
	
    //...
    //执行完毕把数据copy到用户进程
	if (copy_to_user(ubuf, &bwr, sizeof(bwr))) {
		ret = -EFAULT;
		goto out;
	}
out:
	return ret;
}

{% endhighlight %}
**binder_thread_write方法很长，这里省略很多:**
{% highlight c++ %}

static int binder_thread_write(struct binder_proc *proc,struct binder_thread *thread,binder_uintptr_t binder_buffer,
 size_t size,binder_size_t *consumed){
	uint32_t cmd;
	void __user *buffer = (void __user *)(uintptr_t)binder_buffer;
	void __user *ptr = buffer + *consumed;
	void __user *end = buffer + size;

	while (ptr < end && thread->return_error == BR_OK) {
		//getUrser 读出命令 BC_TRANSACTION
		if (get_user(cmd, (uint32_t __user *)ptr))
			return -EFAULT;
		ptr += sizeof(uint32_t);
		//...
		switch (cmd) {
		//...
		case BC_TRANSACTION:
		case BC_REPLY: {
			struct binder_transaction_data tr;
            //copy_from_user 读出binder_transaction_data数据
			if (copy_from_user(&tr, ptr, sizeof(tr)))
				return -EFAULT;
			ptr += sizeof(tr);
			//client请求的时候，主要的逻辑都在这个binder_transaction里面
			binder_transaction(proc, thread, &tr, cmd == BC_REPLY);
			break;
		}

		case BC_REGISTER_LOOPER://注册线程到驱动
			//...
			if (thread->looper & BINDER_LOOPER_STATE_ENTERED) {
			//...
			} else if (proc->requested_threads == 0) {
			//...
			} else {
				proc->requested_threads--;
				proc->requested_threads_started++;
			}
			thread->looper |= BINDER_LOOPER_STATE_REGISTERED;
			break;
		case BC_ENTER_LOOPER://注册线程到驱动
		    //...
			if (thread->looper & BINDER_LOOPER_STATE_REGISTERED) {
				thread->looper |= BINDER_LOOPER_STATE_INVALID;
				//...
			}
			//设置了一个标记状态
			//标记设置为BINDER_LOOPER_STATE_ENTERED
			//binder_thread的一些状态定义在一个枚举里面，binder_driver的数据结构比较复杂，详细可以自己查下
			thread->looper |= BINDER_LOOPER_STATE_ENTERED;
			break;
		case BC_EXIT_LOOPER:
			//...
			thread->looper |= BINDER_LOOPER_STATE_EXITED;
			break;
			//...
		default:
			return -EINVAL;
		}
		*consumed = ptr - buffer;
	}
	return 0;
}

{% endhighlight %}

{% highlight c++ %}



//binder_thread_read方法
//从内核读取数据
//这个方法也很长很长，也省略很多很多
static int binder_thread_read(struct binder_proc *proc,
			      struct binder_thread *thread,
			      binder_uintptr_t binder_buffer, size_t size,
			      binder_size_t *consumed, int non_block){
	void __user *buffer = (void __user *)(uintptr_t)binder_buffer;
	void __user *ptr = buffer + *consumed;
	void __user *end = buffer + size;

	int ret = 0;
	int wait_for_proc_work;//是否需要wait直到有client的请求

	if (*consumed == 0) {
		if (put_user(BR_NOOP, (uint32_t __user *)ptr))
			return -EFAULT;
		ptr += sizeof(uint32_t);
	}

retry:
    //transaction_stack :  正在处理的事务栈
    //thread->todo : 待处理的事务链表
    //都为空 则表示没有事情要做 需要wait
	wait_for_proc_work = thread->transaction_stack == NULL &&
				list_empty(&thread->todo);
    //...
    //标记Binder线程处于空闲状态
	thread->looper |= BINDER_LOOPER_STATE_WAITING;
	//如果线程处于等待状态
	if (wait_for_proc_work)
		proc->ready_threads++;//空闲线程数量++
		
	//...
	//这一段其实就是一个阻塞判断，没有任务的话 就阻塞着，不走下去了，直到有任务来
	if (wait_for_proc_work) {
	    //如果线程还没有注册，则error
		if (!(thread->looper & (BINDER_LOOPER_STATE_REGISTERED |
					BINDER_LOOPER_STATE_ENTERED))) {
		    //...
			wait_event_interruptible(binder_user_error_wait,
						 binder_stop_on_user_error < 2);
		}
		//设置优先级
		binder_set_nice(proc->default_priority);
		//非阻塞走这里
		if (non_block) {
			if (!binder_has_proc_work(proc, thread))
				ret = -EAGAIN;
		} else
		    //阻塞走这里
			ret = wait_event_freezable_exclusive(proc->wait, binder_has_proc_work(proc, thread));
	} else {
		if (non_block) {
			if (!binder_has_thread_work(thread))
				ret = -EAGAIN;
		} else
			ret = wait_event_freezable(thread->wait, binder_has_thread_work(thread));
	}

	binder_lock(__func__);
	//走到这里说明wait结束了，等待的数量--,并去除等待标记
	if (wait_for_proc_work)
		proc->ready_threads--;
	thread->looper &= ~BINDER_LOOPER_STATE_WAITING;

	if (ret)
		return ret;

	while (1) {
		uint32_t cmd;
		struct binder_transaction_data tr;//binder_transaction_data结构体
		struct binder_work *w;//binder_work
		struct binder_transaction *t = NULL;//从binder_work读到的binder_transaction
        
        //线程上的任务不为空则取出以第一个 
		if (!list_empty(&thread->todo)) {
			w = list_first_entry(&thread->todo, struct binder_work,
					     entry);
		//进程上的任务不为空则取出第一个
		} else if (!list_empty(&proc->todo) && wait_for_proc_work) {
			w = list_first_entry(&proc->todo, struct binder_work,
					     entry);
		} else {
			/* no data added */
			//...
		}
        //数据读完就break
		if (end - ptr < sizeof(tr) + 4)
			break;

		switch (w->type) {
		//BINDER_WORK_TRANSACTION write的时候发送的命令
		case BINDER_WORK_TRANSACTION: {
		    //binder_transaction 赋值
			t = container_of(w, struct binder_transaction, work);
		} 
		break;
		//transaction完成
		case BINDER_WORK_TRANSACTION_COMPLETE: {
			cmd = BR_TRANSACTION_COMPLETE;
			//put_user 是 kernel 向用户空间写单个变量,把BR_TRANSACTION_COMPLETE返回给用户进程
			if (put_user(cmd, (uint32_t __user *)ptr))
				return -EFAULT;
			ptr += sizeof(uint32_t);
			binder_stat_br(proc, thread, cmd);
		    //...
			list_del(&w->entry);//删除这个binder_work
			kfree(w);
			binder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);
		} break;
		case BINDER_WORK_NODE: {
		   //...
		} break;
		case BINDER_WORK_DEAD_BINDER:
		case BINDER_WORK_DEAD_BINDER_AND_CLEAR:
		case BINDER_WORK_CLEAR_DEATH_NOTIFICATION: {
		   //...
		} 
		break;
		}
		//case 不走BINDER_WORK_TRANSACTION这t是null 就continue
		if (!t)
			continue;
			
		if (t->buffer->target_node) {
			struct binder_node *target_node = t->buffer->target_node;
			tr.target.ptr = target_node->ptr;//service对象地址
			tr.cookie =  target_node->cookie;
			t->saved_priority = task_nice(current);//优先级
		    //...
			cmd = BR_TRANSACTION;//命令是BR_TRANSACTION
		} else {
			tr.target.ptr = 0;
			tr.cookie = 0;
			cmd = BR_REPLY;
		}
		tr.code = t->code;
		tr.flags = t->flags;
		tr.sender_euid = from_kuid(current_user_ns(), t->sender_euid);
		if (t->from) {
			struct task_struct *sender = t->from->proc->tsk;
			tr.sender_pid = task_tgid_nr_ns(sender,
							task_active_pid_ns(current));
		} else {
			tr.sender_pid = 0;
		}
        //把读出来的binder_transaction赋值给新的binder_transaction_data
		tr.data_size = t->buffer->data_size;
		tr.offsets_size = t->buffer->offsets_size;
		tr.data.ptr.buffer = (binder_uintptr_t)(
					(uintptr_t)t->buffer->data +
					proc->user_buffer_offset);
		tr.data.ptr.offsets = tr.data.ptr.buffer +
					ALIGN(t->buffer->data_size,
					    sizeof(void *));
        //put_user 把 BR_TRANSACTION 命令写给用户空间
		if (put_user(cmd, (uint32_t __user *)ptr))
			return -EFAULT;
		ptr += sizeof(uint32_t);

	    //把binder_transaction_data数据给写给用户空间
		if (copy_to_user(ptr, &tr, sizeof(tr)))
			return -EFAULT;
		ptr += sizeof(tr);

		//...
		binder_stat_br(proc, thread, cmd);
	    //...
		list_del(&t->work.entry);
		t->buffer->allow_user_free = 1;
		if (cmd == BR_TRANSACTION && !(t->flags & TF_ONE_WAY)) {
			t->to_parent = thread->transaction_stack;
			t->to_thread = thread;
			thread->transaction_stack = t;//保存下传送堆栈
		} else {
			t->buffer->transaction = NULL;
			kfree(t);
			binder_stats_deleted(BINDER_STAT_TRANSACTION);
		}
		break;
	}
	//...
	return 0;
}
{% endhighlight %}

1.创建ActivityManagerService   
==

ActivityManagerService.java
==

①  
==

**frameworks\base\services\java\com\android\server\am\ActivityManagerService.java**
{% highlight java %}

public final class ActivityManagerService extends ActivityManagerNative
        implements Watchdog.Monitor, BatteryStatsImpl.BatteryCallback {
        
        //***
        
        }

{% endhighlight %}
ActivityManagerNative.java
==

②
==

**ActivityManagerService继承自ActivityManagerNative**
**frameworks\base\core\java\android\app\ActivityManagerNative.java**
{% highlight java %}

public abstract class ActivityManagerNative extends Binder implements IActivityManager{

}

{% endhighlight %}

Binder.java
==   

③
==

**ActivityManagerNative又继承Binder。**
**frameworks\base\core\java\android\os\Binder.java**
{% highlight java %}

public class Binder implements IBinder {
     private int mObject;//用来保存native层的指针
      //构造方法
      public Binder() {
        init();
       //***
    }
    
    //native的init方法
    private native final void init();
}

{% endhighlight %}
④
==
ActivityManagerService的实例化是在SystemServer中调用的；
{% highlight java %}

 ActivityManagerService m = new ActivityManagerService();

{% endhighlight %}
调用构造方法自然会先调用父类的构造方法，会调用到Binder.java，继而调用init();
init();是一个native方法，它的实现在android_util_Binder.cpp；
android_util_Binder.cpp
==
⑤
==
**frameworks\base\core\jni\android_util_Binder.cpp**
Binder.java->init对应的方法是android_os_Binder_init;
{% highlight c++ %}

static void android_os_Binder_init(JNIEnv* env, jobject obj){
    //[1]
    JavaBBinderHolder* jbh = new JavaBBinderHolder();
    //***
    jbh->incStrong((void*)android_os_Binder_init);
    //[2] gBinderOffsets
    env->SetIntField(obj, gBinderOffsets.mObject, (int)jbh);
}

{% endhighlight %}
⑥
==
**[1] 第一步new JavaBBinderHolder();**
**JavaBBinderHolder也在android_util_Binder.cpp这个文件里面**
{% highlight c++ %}

class JavaBBinderHolder : public RefBase{
public:
    sp<JavaBBinder> get(JNIEnv* env, jobject obj){
        AutoMutex _l(mLock);
        sp<JavaBBinder> b = mBinder.promote();
        if (b == NULL) {
            b = new JavaBBinder(env, obj);
            mBinder = b;
        }
        return b;
    }

    sp<JavaBBinder> getExisting(){
        AutoMutex _l(mLock);
        return mBinder.promote();
    }

private:
    Mutex           mLock;
    wp<JavaBBinder> mBinder;
};

{% endhighlight %}
⑦
==
**gBinderOffsets**是一个结构体,用来保存Binder.java的一些fieldID和MethodID，在注册jni的时候会初始化
{% highlight c++ %}

//注册Binder系列的jni信息
int register_android_os_Binder(JNIEnv* env)
{
    if (int_register_android_os_Binder(env) < 0)
        return -1;
    if (int_register_android_os_BinderInternal(env) < 0)
        return -1;
    if (int_register_android_os_BinderProxy(env) < 0)
        return -1;
    if (int_register_android_os_Parcel(env) < 0)
        return -1;
    return 0;
}


const char* const kBinderPathName = "android/os/Binder";

static int int_register_android_os_Binder(JNIEnv* env){
    jclass clazz;
    clazz = env->FindClass(kBinderPathName);//找到Binder.java的class信息
    gBinderOffsets.mClass = (jclass) env->NewGlobalRef(clazz);
    gBinderOffsets.mExecTransact
        = env->GetMethodID(clazz, "execTransact", "(IIII)Z");
    assert(gBinderOffsets.mExecTransact);
    //把Binder.java的mObject fieldID存起来
    gBinderOffsets.mObject= env->GetFieldID(clazz, "mObject", "I");
    assert(gBinderOffsets.mObject);
    return AndroidRuntime::registerNativeMethods(
        env, kBinderPathName,
        gBinderMethods, NELEM(gBinderMethods));
}


{% endhighlight %}
在app_process启动的时候会注册一些jni函数以及绑定关联，具体可以了解一下Android启动的逻辑。

⑧
==
**[2]** 第二步把JavaBBinderHolder对象的指针绑定在Binder.java的mObject上面
gBinderOffsets.mObject对应的就是Binder.java的mObject。
再来看Binder.java
{% highlight java %}

public class Binder implements IBinder {
     private int mObject;//native层JavaBBinderHolder的指针
      //构造方法
      public Binder() {
        init();
       //***
    }
    
    //native的init方法
    private native final void init();
}

{% endhighlight %}

**ActivityManagerService的创建基本到这里结束了。**
2、注册到ServiceManager
==
⑨
==
ActivityManager在SystemServer中初始化之后,就需要注册到ServiceManager了。
{% highlight java %}

    //SystemServer调用
    ActivityManagerService.setSystemProcess();

    public static void setSystemProcess() {
            ActivityManagerService m = mSelf;
            //【1】注册到ServiceManager
            ServiceManager.addService("activity", m);
            //***省了很多代码
    }

{% endhighlight %}

ServiceManager
==
ServiceManager管理着系统中所有的服务，ActivityManagerService作为Server端需要向ServiceManager注册，以供其他Client进程来getService,调用。

⑩
==
**ServiceManager**
**frameworks\base\core\java\android\os\ServiceManager.java**
{% highlight java %}

public final class ServiceManager {
    private static final String TAG = "ServiceManager";
    private static IServiceManager sServiceManager;
    
    //添加注册其他Service
    public static void addService(String name, IBinder service) {
        try {
            //【11】添加到ServiceManager
            getIServiceManager().addService(name, service);
        } catch (RemoteException e) {
            Log.e(TAG, "error in addService", e);
        }
    }

{% endhighlight %}
⑪
==
**getIServiceManager返回的对象是什么,为什么调用它的addService**
{% highlight java %}

  private static IServiceManager getIServiceManager() {
        if (sServiceManager != null) {
            return sServiceManager;
        }

        // Find the service manager
        sServiceManager = ServiceManagerNative.asInterface(BinderInternal.getContextObject());
        return sServiceManager;
    }

{% endhighlight %}
**上面的代码可以分成几步:**
{% highlight java %}

//【1】
IBinder obj = BinderInternal.getContextObject();
//【2】
ServiceManagerNative.asInterface(obj);

{% endhighlight %}
⑫
==
**先看【1】BinderInternal.getContextObject()**
**frameworks\base\core\java\com\android\internal\os\BinderInternal.java**
{% highlight java %}

//返回一个IBinder对象,native方法对应的方法是android_os_BinderInternal_getContextObject
public static final native IBinder getContextObject();

{% endhighlight %}
**转到android_util_Binder.cpp**
{% highlight java %}

static jobject android_os_BinderInternal_getContextObject(JNIEnv* env, jobject clazz){
    //调用ProcessState的getContextObject
    sp<IBinder> b = ProcessState::self()->getContextObject(NULL);
    return javaObjectForIBinder(env, b);
}

{% endhighlight %}

⑬
==
**ProcessState.getContextObject(NULL);**
{% highlight java %}

sp<IBinder> ProcessState::getContextObject(const sp<IBinder>& caller){
    return getStrongProxyForHandle(0);
}

//handle 代表service的标示号
sp<IBinder> ProcessState::getStrongProxyForHandle(int32_t handle){
    sp<IBinder> result;
    AutoMutex _l(mLock);
    
    //查找是否有handle_entry没有就新建一个
    handle_entry* e = lookupHandleLocked(handle);
    if (e != NULL) {
    
        //第一次调用的时候 b是null
        IBinder* b = e->binder;
        if (b == NULL || !e->refs->attemptIncWeak(this)) {
        
           //【1】 new一个BpBinder
            b = new BpBinder(handle); 
            e->binder = b;
            if (b) e->refs = b->getWeakRefs();
            result = b;
        } else {
            result.force_set(b);
            e->refs->decWeak(this);
        }
    }
    return result;
}

{% endhighlight %}
⑭
==
**先来看下BpBinder**
{% highlight java %}

//继承自IBinder
class BpBinder : public IBinder{
public:
                        BpBinder(int32_t handle);

    inline  int32_t     handle() const { return mHandle; }

    virtual const String16&    getInterfaceDescriptor() const;
    virtual bool        isBinderAlive() const;
    virtual status_t    pingBinder();
    virtual status_t    dump(int fd, const Vector<String16>& args);

    virtual status_t    transact(   uint32_t code,
                                    const Parcel& data,
                                    Parcel* reply,
                                    uint32_t flags = 0);

    virtual status_t    linkToDeath(const sp<DeathRecipient>& recipient,
                                    void* cookie = NULL,
                                    uint32_t flags = 0);
    virtual status_t    unlinkToDeath(  const wp<DeathRecipient>& recipient,
                                        void* cookie = NULL,
                                        uint32_t flags = 0,
                                        wp<DeathRecipient>* outRecipient = NULL);

    virtual void        attachObject(   const void* objectID,
                                        void* object,
                                        void* cleanupCookie,
                                        object_cleanup_func func);
    virtual void*       findObject(const void* objectID) const;
    virtual void        detachObject(const void* objectID);

    virtual BpBinder*   remoteBinder();

            status_t    setConstantData(const void* data, size_t size);
            void        sendObituary();
}

{% endhighlight %}
go to ⑫
==
{% highlight java %}

static jobject android_os_BinderInternal_getContextObject(JNIEnv* env, jobject clazz){
    //调用ProcessState的getContextObject,返回一个BpBinder对象
    sp<IBinder> b = ProcessState::self()->getContextObject(NULL);
    return javaObjectForIBinder(env, b);
}

{% endhighlight %}
**先看下BinderProxy.java,定义在Binder.java文件中**
{% highlight java %}

final class BinderProxy implements IBinder {
    public native boolean pingBinder();
    public native boolean isBinderAlive();
    final private WeakReference mSelf;
    private int mObject;//native层BpBinder的指针
    private int mOrgue;
    
    //返回null
    public IInterface queryLocalInterface(String descriptor) {
        return null;
    }
}

{% endhighlight %}

⑮
==
**再看javaObjectForIBinder(env，b);**
{% highlight java %}

//create一个BinderProxy.java对象
jobject javaObjectForIBinder(JNIEnv* env, const sp<IBinder>& val){
     //val == BpBinder
     
     //BpBinder没有实现checkSubclass,默认IBinder实现返回false
    if (val->checkSubclass(&gBinderOffsets)) {
        jobject object = static_cast<JavaBBinder*>(val.get())->object();
        return object;
    }

    AutoMutex _l(mProxyLock);
    //gBinderProxyOffsets这里对应的是"android.os.BinderProxy.java"
    //gBinderProxyOffsets这个对应信息跟之前Binder的jni注册一样，在启动的时候注册了
    //查找ObjectManager中有没有这个java对象
    jobject object = (jobject)val->findObject(&gBinderProxyOffsets);
    if (object != NULL) {
        jobject res = env->CallObjectMethod(object, gWeakReferenceOffsets.mGet);
        if (res != NULL) {
            return res;
        }
        android_atomic_dec(&gNumProxyRefs);
        val->detachObject(&gBinderProxyOffsets);
        env->DeleteGlobalRef(object);
    }
    //创建一个BinderProxy.java对象
    object = env->NewObject(gBinderProxyOffsets.mClass, gBinderProxyOffsets.mConstructor);
    if (object != NULL) {
        // The proxy holds a reference to the native object.
        //把BpBinder的指针绑定在java的BinderProxy.mObject上
        env->SetIntField(object, gBinderProxyOffsets.mObject, (int)val.get());
        val->incStrong(object);

        // The native object needs to hold a weak reference back to the
        // proxy, so we can retrieve the same proxy if it is still active.
        //BpBinder持有一个BinderProxy的弱引用
        jobject refObject = env->NewGlobalRef(
                env->GetObjectField(object, gBinderProxyOffsets.mSelf));
        val->attachObject(&gBinderProxyOffsets, refObject,
                jnienv_to_javavm(env), proxy_cleanup);

        // Also remember the death recipients registered on this proxy
        sp<DeathRecipientList> drl = new DeathRecipientList;
        drl->incStrong((void*)javaObjectForIBinder);
        env->SetIntField(object, gBinderProxyOffsets.mOrgue, reinterpret_cast<jint>(drl.get()));

        // Note that a new object reference has been created.
        android_atomic_inc(&gNumProxyRefs);
        incRefsCreated(env);
    }

    return object;//返回java层，object对象就是一个BinderProxy对象
}

{% endhighlight %}
go to ⑫
==
**由此可见**
{% highlight java %}

//getContext返回的是一个BinderProxy对象
BinderProxy obj = BinderInternal.getContextObject();
//【2】
ServiceManagerNative.asInterface(obj);

{% endhighlight %}
⑯
==
**ServiceManagerNative.asInterface(BpBinder);**
{% highlight java %}

  static public IServiceManager asInterface(IBinder obj){
        //这里obj的类型就是一个BinderProxy
        if (obj == null) {
            return null;
        }
         //BinderProxy.queryLocalInterface(descriptor);                             
         //返回null，可以看上面BinderProxy
        IServiceManager in =(IServiceManager)obj.queryLocalInterface(descriptor);
        if (in != null) {
            return in;
        }
        //终于找到了addService的调用者
        return new ServiceManagerProxy(obj);
    }

{% endhighlight %}
go to ⑩
==

{% highlight java %}

  getIServiceManager().addService(name, service);
        //等价于
  ServiceManagerProxy.addService(name,service);    

{% endhighlight %}
⑰
==
**来看看ServiceManagerProxy.java**
**frameworks\base\core\java\android\os\ServiceManagerNative.java**
{% highlight java %}

class ServiceManagerProxy implements IServiceManager {

    private IBinder mRemote;//BinderProxy.java
    
    public ServiceManagerProxy(IBinder remote) {
        mRemote = remote;
    }

    public IBinder asBinder() {
        return mRemote;
    }
    
      //***省了很多代码
    
    public void addService(String name, IBinder service)
            throws RemoteException {
        Parcel data = Parcel.obtain();
        Parcel reply = Parcel.obtain();
                                //"android.os.IServiceManager"
        data.writeInterfaceToken(IServiceManager.descriptor);
        data.writeString(name);//"activity" 服务的名称
        data.writeStrongBinder(service);//ActivityManagerService
        //mRemote == BinderProxy
        mRemote.transact(ADD_SERVICE_TRANSACTION, data, reply, 0);
        reply.recycle();
        data.recycle();
    }
      //*** 省了很多代码
}
    

{% endhighlight %}
⑱
==
**下一步就在BinderProxy.transact**
{% highlight java %}

final class BinderProxy implements IBinder {
    //没错又是一个native->android_os_BinderProxy_transact
    public native boolean transact(int code, Parcel data, Parcel reply,
            int flags) throws RemoteException;
}

{% endhighlight %}
**依然在android_util_Binder.cpp中定义**
{% highlight java %}

static jboolean android_os_BinderProxy_transact(JNIEnv* env, jobject obj,
        jint code, jobject dataObj, jobject replyObj, jint flags) // throws RemoteException{
    //...
    //转成自己的Parcel
    Parcel* data = parcelForJavaObject(env, dataObj);
    //...
    Parcel* reply = parcelForJavaObject(env, replyObj);
    //...
    //target == BpBinder,之前new BinderProxy的时候绑定的BpBinder
    IBinder* target = (IBinder*)env->GetIntField(obj, gBinderProxyOffsets.mObject);
    //...
    const bool time_binder_calls = should_time_binder_calls();

    int64_t start_millis;
    if (time_binder_calls) {
        start_millis = uptimeMillis();
    }
    
    //实际上就是BpBinder->transact。
    status_t err = target->transact(code, *data, reply, flags);

    if (time_binder_calls) {
        conditionally_log_binder_call(start_millis, target, code);
    }

    if (err == NO_ERROR) {
        return JNI_TRUE;
    } else if (err == UNKNOWN_TRANSACTION) {
        return JNI_FALSE;
    }

    signalExceptionForError(env, obj, err, true /*canThrowRemoteException*/);
    return JNI_FALSE;
}

{% endhighlight %}
⑲
==
**转到BpBinder**
{% highlight java %}

status_t BpBinder::transact(
    uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags){
    if (mAlive) {
    
        //又转交给IPCThreadState->transact
        status_t status = IPCThreadState::self()->transact(
            mHandle, code, data, reply, flags);
        if (status == DEAD_OBJECT){
            mAlive = 0;
        }
        return status;
    }

    return DEAD_OBJECT;
}

{% endhighlight %}
⑳
==
**IPCThreadState->transact**
{% highlight java %}

status_t IPCThreadState::transact(int32_t handle,
                                  uint32_t code, const Parcel& data,
                                  Parcel* reply, uint32_t flags){
    status_t err = data.errorCheck();

    flags |= TF_ACCEPT_FDS;
    //...
    if (err == NO_ERROR) {
        //准备要交易的数据结构 Binder驱动所需的数据
        //BC_TRANSACTION : 应用发给Binder内核驱动的命令,代表业务命令
        err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL);
    }
    //error
    if (err != NO_ERROR) {
        if (reply) reply->setError(err);
        return (mLastError = err);
    }
    //TF_ONE_WAY为0表示是同步请求
    if ((flags & TF_ONE_WAY) == 0) {
        //...
        if (reply) {
            //开始交易，等待回复,replay用来装数据
            err = waitForResponse(reply);
        } else {
            Parcel fakeReply;
            err = waitForResponse(&fakeReply);
        }
         //...
    } else {
        err = waitForResponse(NULL, NULL);
    }
    
    return err;
}
{% endhighlight %}

{% highlight java %}

status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t binderFlags,
    int32_t handle, uint32_t code, const Parcel& data, status_t* statusBuffer){
    //交易的结构体
    binder_transaction_data tr;

    tr.target.handle = handle;//service的标识号 这里应该是0
    tr.code = code;//ADD_SERVICE_TRANSACTION
    tr.flags = binderFlags;
    tr.cookie = 0;
    tr.sender_pid = 0;
    tr.sender_euid = 0;
    
    const status_t err = data.errorCheck();
    if (err == NO_ERROR) {
        tr.data_size = data.ipcDataSize();
        tr.data.ptr.buffer = data.ipcData();
        tr.offsets_size = data.ipcObjectsCount()*sizeof(size_t);
        tr.data.ptr.offsets = data.ipcObjects();
    } else if (statusBuffer) {
      //...
    } else {
        return (mLastError = err);
    }
    //这里的cmd是BC_TRANSACTION
    mOut.writeInt32(cmd);
    //写入到Parcel
    mOut.write(&tr, sizeof(tr));
    return NO_ERROR;
}

{% endhighlight %}
㉑
==
**waitForResponse**
{% highlight java %}

status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult){
    int32_t cmd;
    int32_t err;

    while (1) {
        //talkWidthDriver,交易处理在这里面,跟内核通信
        if ((err=talkWithDriver()) < NO_ERROR) break;
        err = mIn.errorCheck();
        if (err < NO_ERROR) break;
        //一直循环直到有数据可以读取
        if (mIn.dataAvail() == 0) continue;
        ///读取mIn的整数字段，存放的是Binder Return指令，以BR_为前缀
        cmd = mIn.readInt32();
        //...
        switch (cmd) {
        //BR_TRANSACTION_COMPLETE 是 kernel binder 告诉调用者，
        //命令发送已经处理完毕（前面 client 有对 kernel 发送 BC_TRANSACTION 命令）。
        case BR_TRANSACTION_COMPLETE://结束
            if (!reply && !acquireResult) goto finish;
            break;
        
        case BR_DEAD_REPLY://目标进程/线程/binder实体为空, 以及释放正在等待reply的binder thread或者binder buffer
            err = DEAD_OBJECT;
            goto finish;

        case BR_FAILED_REPLY://情况较多,比如非法handle, 错误事务栈, security, 内存不足, buffer不足, 数据拷贝失败, 节点创建失败, 各种不匹配等问题
            err = FAILED_TRANSACTION;
            goto finish;
        
         //...
        case BR_REPLY://接收返回的数据{
                binder_transaction_data tr;
                err = mIn.read(&tr, sizeof(tr));
               //...
               //读取数据 看不懂写的什么
                if (reply) {
                    if ((tr.flags & TF_STATUS_CODE) == 0) {
                        reply->ipcSetDataReference(
                            reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),
                            tr.data_size,
                            reinterpret_cast<const size_t*>(tr.data.ptr.offsets),
                            tr.offsets_size/sizeof(size_t),
                            freeBuffer, this);
                    } else {
                        err = *static_cast<const status_t*>(tr.data.ptr.buffer);
                        freeBuffer(NULL,
                            reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),
                            tr.data_size,
                            reinterpret_cast<const size_t*>(tr.data.ptr.offsets),
                            tr.offsets_size/sizeof(size_t), this);
                    }
                } else {
                    freeBuffer(NULL,
                        reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),
                        tr.data_size,
                        reinterpret_cast<const size_t*>(tr.data.ptr.offsets),
                        tr.offsets_size/sizeof(size_t), this);
                    continue;
                }
            }
            goto finish;

        default:
            err = executeCommand(cmd);
            if (err != NO_ERROR) goto finish;
            break;
        }
    }

finish:
    if (err != NO_ERROR) {
        if (acquireResult) *acquireResult = err;
        if (reply) reply->setError(err);
        mLastError = err;
    }
    
    return err;
}

{% endhighlight %}
**talkWithDriver与内核通信**
{% highlight java %}

status_t IPCThreadState::talkWithDriver(bool doReceive){

    binder_write_read bwr;//BINDER_WRITE_READ指令需要携带的数据结构

	//dataSize表示当前存储的字节数，存储一个int占4字节，存储一个long占8字节。
    //dataPosition表示当前需要读取的数据的索引位置，是4的整数倍。
    //当dataPosition＞=dataSize时，说明已经处理完上次的返回数据*/
    const bool needRead = mIn.dataPosition() >= mIn.dataSize();
    
    // We don't want to write anything if we are still reading
    // from data left in the input buffer and the caller
    // has requested to read the next data.
     //只有不需要读取数据的时候 才发送数据                   
    const size_t outAvail = (!doReceive || needRead) ? mOut.dataSize() : 0;
    
    bwr.write_size = outAvail;
    bwr.write_buffer = (long unsigned int)mOut.data();

    // This is what we'll read.
    //需要接收数据&&上一次已经读完了
    if (doReceive && needRead) {
        bwr.read_size = mIn.dataCapacity();
        bwr.read_buffer = (long unsigned int)mIn.data();
    } else {
        bwr.read_size = 0;
        bwr.read_buffer = 0;
    }

    //...    
    // Return immediately if there is nothing to do.
    //如果不需要写 也不需要读 就立即返回 
    if ((bwr.write_size == 0) && (bwr.read_size == 0)) return NO_ERROR;

    bwr.write_consumed = 0;
    bwr.read_consumed = 0;
    status_t err;
    do {
      //...
#if defined(HAVE_ANDROID_OS)
        //通过ioctl BINDER_WRITE_READ指令与Binder驱动通信
        //这个内核会找到目标service对应的进程，进行传输数据，当前就会调用到serviceManager,由serviceManager处理
        if (ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)
            err = NO_ERROR;
        else
            err = -errno;
#else
        err = INVALID_OPERATION;
    //...
    } while (err == -EINTR);
    //...

    if (err >= NO_ERROR) {
		////清理输出缓冲区中已处理的数据
        if (bwr.write_consumed > 0) {
            if (bwr.write_consumed < (ssize_t)mOut.dataSize())
                mOut.remove(0, bwr.write_consumed);
            else
                mOut.setDataSize(0);
        }
        //从Binder驱动中返回的数据保存在mIn中，这里设置读取位置
        if (bwr.read_consumed > 0) {
            mIn.setDataSize(bwr.read_consumed);//设置输入缓冲区可读数据大小
            mIn.setDataPosition(0);//将读取位置设置为开始位置
        }
        //...
        return NO_ERROR;
    }
    
    return err;
}

{% endhighlight %}
**talkWithDriver**调用ioctl跟serviceManager那里一样也会调用到BinderDriver的**binder_ioctl_write_read**方法。
{% highlight java %}

static int binder_thread_write(struct binder_proc *proc,
			struct binder_thread *thread,
			binder_uintptr_t binder_buffer, size_t size,
			binder_size_t *consumed){
			
			//...
//上面的cmd是BC_TRANSACTION			
 case BC_TRANSACTION:
 case BC_REPLY: {
            struct binder_transaction_data tr;
            //copy_from_user 读出binder_transaction_data数据
            if (copy_from_user(&tr, ptr, sizeof(tr)))
                return -EFAULT;
            ptr += sizeof(tr);
            //client请求的时候，主要的逻辑都在这个binder_transaction里面
            binder_transaction(proc, thread, &tr, cmd == BC_REPLY);
            break;
        }
    }

{% endhighlight %}
可以看到对事件的处理都在**binder_transaction**方法里面;这个方法也超级长，省略了很多   
{% highlight c++ %}

static void binder_transaction(struct binder_proc *proc,
			       struct binder_thread *thread,
			       struct binder_transaction_data *tr, int reply){
	//每个transact() 调用在内核里都会生产一个binder_transaction 对象，
	//这个对象会最终送到Service进程或线程的todo队列里，然后唤醒他们来最终完成onTransact()调用。
	struct binder_transaction *t;
	//一个进程产生的请求会变成一个binder_work,
	//并送入目标进程或线程的todo 队列里，然后唤醒目标进程和线程来完成这个请求
	struct binder_work *tcomplete;//完成的binder_work
	binder_size_t *offp, *off_end;
	binder_size_t off_min;
	struct binder_proc *target_proc;//目标server所在进程的结构体
	struct binder_thread *target_thread = NULL;//目标server的binder_thread
	struct binder_node *target_node = NULL;//目标server的binder_node
	struct list_head *target_list;//目标server的todo队列 
	wait_queue_head_t *target_wait;
	struct binder_transaction *in_reply_to = NULL;
	struct binder_transaction_log_entry *e;
	uint32_t return_error;

	e = binder_transaction_log_add(&binder_transaction_log);
	e->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);
	e->from_proc = proc->pid;
	e->from_thread = thread->pid;
	e->target_handle = tr->target.handle;
	e->data_size = tr->data_size;
	e->offsets_size = tr->offsets_size;
    //如果是BC_REPLY,server处理完回复的时候走这个分支
	if (reply) {
		in_reply_to = thread->transaction_stack;//之前保存transaction_stack
	    //...
		binder_set_nice(in_reply_to->saved_priority);//保存的优先级
		//...
		thread->transaction_stack = in_reply_to->to_parent;
		target_thread = in_reply_to->from;//client的调用线程
	    //...
		target_proc = target_thread->proc;//client所在的进程结构
	} else {   //非BC_REPLY
        //handle大于0则代表其他服务
		if (tr->target.handle) {
			struct binder_ref *ref;
            //根据handle来找到对应的binder_ref
			ref = binder_get_ref(proc, tr->target.handle, true);
		    //...
			target_node = ref->node;
		} else {
		    //handle == 0就是ServiceManager
			target_node = binder_context_mgr_node;
			//...
		}
		e->to_node = target_node->debug_id;
		//找到目标service的binder_proc
		target_proc = target_node->proc;
		//...
		//...
		if (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {
			struct binder_transaction *tmp;
			tmp = thread->transaction_stack;
		    //...
			while (tmp) {
				if (tmp->from && tmp->from->proc == target_proc)
					target_thread = tmp->from;
				tmp = tmp->from_parent;
			}
		}
	}
	//如果目标server的target_thread不为空，则获取pid、todo链表
	if (target_thread) {
		e->to_thread = target_thread->pid;
		target_list = &target_thread->todo;
		target_wait = &target_thread->wait;
	} else {
	    //否则就获取目标server的jin
		target_list = &target_proc->todo;
		target_wait = &target_proc->wait;
	}
	e->to_proc = target_proc->pid;

	/* TODO: reuse incoming transaction for reply */
	t = kzalloc(sizeof(*t), GFP_KERNEL);
	//...
	binder_stats_created(BINDER_STAT_TRANSACTION);
	tcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);
    //...
	binder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);
	t->debug_id = ++binder_last_id;
	e->debug_id = t->debug_id;

	//...
	//组装binder_transaction的一些信息
	if (!reply && !(tr->flags & TF_ONE_WAY))
		t->from = thread;
	else
		t->from = NULL;
	t->sender_euid = task_euid(proc->tsk);
	t->to_proc = target_proc;//目标进程的信息
	t->to_thread = target_thread;//目标server的binder_thread
	t->code = tr->code;//code
	t->flags = tr->flags;
	t->priority = task_nice(current);//优先级
    //...
    //传输的数据
	t->buffer = binder_alloc_buf(target_proc, tr->data_size,
		tr->offsets_size, !reply && (t->flags & TF_ONE_WAY));
	//...
	t->buffer->allow_user_free = 0;
	t->buffer->debug_id = t->debug_id;
	t->buffer->transaction = t;//transaction
	t->buffer->target_node = target_node;//binder_node
	trace_binder_transaction_alloc_buf(t->buffer);
	if (target_node)
		binder_inc_node(target_node, 1, 0, NULL);

	offp = (binder_size_t *)(t->buffer->data +
				 ALIGN(tr->data_size, sizeof(void *)));

    //...
	off_end = (void *)offp + tr->offsets_size;
	off_min = 0;
	for (; offp < off_end; offp++) {
		struct flat_binder_object *fp;
		//...
		fp = (struct flat_binder_object *)(t->buffer->data + *offp);
		off_min = *offp + sizeof(struct flat_binder_object);
		switch (fp->type) {
		case BINDER_TYPE_BINDER:
		case BINDER_TYPE_WEAK_BINDER: {
			struct binder_ref *ref;
			struct binder_node *node = binder_get_node(proc, fp->binder);
			//...
			//从目标进程里面查找当前的binder_node对应的bind_ref对象,没有找到会创建一个并保存起来，
			//binder server的handle也是在这里生成的,具体可以自己看看里面的实现
			ref = binder_get_ref_for_node(target_proc, node);
			//...
			if (fp->type == BINDER_TYPE_BINDER)
				fp->type = BINDER_TYPE_HANDLE;
			else
				fp->type = BINDER_TYPE_WEAK_HANDLE;
			fp->binder = 0;
			fp->handle = ref->desc;//binder handle的赋值
			fp->cookie = 0;
			binder_inc_ref(ref, fp->type == BINDER_TYPE_HANDLE,
				       &thread->todo);
			//...
		} 
		break;
		case BINDER_TYPE_HANDLE:
		case BINDER_TYPE_WEAK_HANDLE: {
			struct binder_ref *ref;
			ref = binder_get_ref(proc, fp->handle,
					     fp->type == BINDER_TYPE_HANDLE);

			if (ref->node->proc == target_proc) {
				if (fp->type == BINDER_TYPE_HANDLE)
					fp->type = BINDER_TYPE_BINDER;
				else
					fp->type = BINDER_TYPE_WEAK_BINDER;
				fp->binder = ref->node->ptr;
				fp->cookie = ref->node->cookie;
				binder_inc_node(ref->node, fp->type == BINDER_TYPE_BINDER, 0, NULL);
				//...
			} else {
				struct binder_ref *new_ref;
				new_ref = binder_get_ref_for_node(target_proc, ref->node);
				//...
				fp->binder = 0;
				fp->handle = new_ref->desc;
				fp->cookie = 0;
				binder_inc_ref(new_ref, fp->type == BINDER_TYPE_HANDLE, NULL);
			    //...
			}
		} 
		break;
        //表示传递的是文件形式的Binder
		case BINDER_TYPE_FD: {
			int target_fd;
			struct file *file;
			if (reply) {
			//...
			file = fget(fp->handle);
			//...
			target_fd = task_get_unused_fd_flags(target_proc, O_CLOEXEC);
			//...
			task_fd_install(target_proc, target_fd, file);
			//...
			fp->binder = 0;
			fp->handle = target_fd;
		} break;
		//...
	}
	
	if (reply) {
	    //...
	    //如果是回复消息，则把之前 client 保存堆栈出栈
		binder_pop_transaction(target_thread, in_reply_to);
	} else if (!(t->flags & TF_ONE_WAY)) {
		//...
		t->need_reply = 1;
		t->from_parent = thread->transaction_stack;
		thread->transaction_stack = t;
	} else {
		//...
		if (target_node->has_async_transaction) {
			target_list = &target_node->async_todo;
			target_wait = NULL;
		} else
			target_node->has_async_transaction = 1;
	}
	//client与server 对于内核来说都是一样的用户进程，这里逻辑一样。
	t->work.type = BINDER_WORK_TRANSACTION;//设置binder_transacetion的work type
	list_add_tail(&t->work.entry, target_list);//把binder_work加入到目标server的todo队列
	tcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;//把自己的binder_work设置为完成
	list_add_tail(&tcomplete->entry, &thread->todo);//把完成的binder_work加入到自己的todo链表
	if (target_wait) {
		if (reply || !(t->flags & TF_ONE_WAY))
			wake_up_interruptible_sync(target_wait);
		else
			wake_up_interruptible(target_wait);//唤醒target_node 进程。到这里完成了进程的切换。
	}
	return;
//...

{% endhighlight  %}
client已经把请求发送出去了，继续看client这边,内核给client的线程插入了一个BR_TRANSACTION_COMPLETE的binder_work告诉client，请求发送完成，然后client下一次talkwithdriver的时候，没有todo队列了，就wait了。

由于之前server端wait了，这时候插入了一个binder_work,唤醒了server端。
## 参考资料: ##
[Android Binder 分析——通信模型](http://light3moon.com/2015/01/28/Android%20Binder%20%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%9E%8B/)    
[图解Android - Binder 和 Service](http://www.cnblogs.com/samchen2009/p/3316001.html)
[Binder驱动笔记](https://blog.csdn.net/guoqifa29/article/details/43702193)
[Binder框架 – 用户空间和驱动的交互](https://blog.csdn.net/chituhuan/article/details/53619233)
[Android Binder分析---内存管理](http://light3moon.com/2015/01/28/Android%20Binder%20%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/)